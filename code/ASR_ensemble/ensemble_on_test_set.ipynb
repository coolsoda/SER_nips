{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/480 records in session Ses05E\n",
      "Processed 20/480 records in session Ses05E\n",
      "Processed 30/480 records in session Ses05E\n",
      "Processed 40/480 records in session Ses05E\n",
      "Processed 50/480 records in session Ses05E\n",
      "Processed 60/480 records in session Ses05E\n",
      "Processed 70/480 records in session Ses05E\n",
      "Processed 80/480 records in session Ses05E\n",
      "Processed 90/480 records in session Ses05E\n",
      "Processed 100/480 records in session Ses05E\n",
      "Processed 110/480 records in session Ses05E\n",
      "Processed 120/480 records in session Ses05E\n",
      "Processed 130/480 records in session Ses05E\n",
      "Processed 140/480 records in session Ses05E\n",
      "Processed 150/480 records in session Ses05E\n",
      "Processed 160/480 records in session Ses05E\n",
      "Processed 170/480 records in session Ses05E\n",
      "Processed 180/480 records in session Ses05E\n",
      "Processed 190/480 records in session Ses05E\n",
      "Processed 200/480 records in session Ses05E\n",
      "Processed 210/480 records in session Ses05E\n",
      "Processed 220/480 records in session Ses05E\n",
      "Processed 230/480 records in session Ses05E\n",
      "Processed 240/480 records in session Ses05E\n",
      "Processed 250/480 records in session Ses05E\n",
      "Processed 260/480 records in session Ses05E\n",
      "Processed 270/480 records in session Ses05E\n",
      "Processed 280/480 records in session Ses05E\n",
      "Processed 290/480 records in session Ses05E\n",
      "Processed 300/480 records in session Ses05E\n",
      "Processed 310/480 records in session Ses05E\n",
      "Processed 320/480 records in session Ses05E\n",
      "Processed 330/480 records in session Ses05E\n",
      "Processed 340/480 records in session Ses05E\n",
      "Processed 350/480 records in session Ses05E\n",
      "Processed 360/480 records in session Ses05E\n",
      "Processed 370/480 records in session Ses05E\n",
      "Processed 380/480 records in session Ses05E\n",
      "Processed 390/480 records in session Ses05E\n",
      "Processed 400/480 records in session Ses05E\n",
      "Processed 410/480 records in session Ses05E\n",
      "Processed 420/480 records in session Ses05E\n",
      "Processed 430/480 records in session Ses05E\n",
      "Processed 440/480 records in session Ses05E\n",
      "Processed 450/480 records in session Ses05E\n",
      "Processed 460/480 records in session Ses05E\n",
      "Processed 470/480 records in session Ses05E\n",
      "Processed 480/480 records in session Ses05E\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "import time\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-'\n",
    "\n",
    "client = OpenAI()\n",
    "log = []\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(10))\n",
    "def gpt_request(**kwargs):\n",
    "    return client.chat.completions.create(**kwargs)\n",
    "\n",
    "def get_filtered_transcriptions(sentence, min_length=5):\n",
    "    transcriptions = [\n",
    "        sentence['hubertlarge'],\n",
    "        sentence['w2v2100'],\n",
    "        sentence['w2v2960'],\n",
    "        sentence['w2v2960large'],\n",
    "        sentence['w2v2960largeself'],\n",
    "        sentence['wavlmplus'],\n",
    "        sentence['whisperbase'],\n",
    "        sentence['whisperlarge'],\n",
    "        sentence['whispermedium'],\n",
    "        sentence['whispersmall'],\n",
    "        sentence['whispertiny']\n",
    "    ]\n",
    "    \n",
    "    # Filter out very short transcriptions first\n",
    "    filtered_transcriptions = [trans for trans in transcriptions if len(trans) > min_length]\n",
    "    \n",
    "    # If all transcriptions are short, return all available transcriptions\n",
    "    if not filtered_transcriptions:\n",
    "        return transcriptions\n",
    "    \n",
    "    return filtered_transcriptions\n",
    "\n",
    "def refine_transcription(transcriptions):\n",
    "    prompt = \"Choose the most comprehensive and coherent sentence from the following options. If impossible to decide, choose the longest option available. Output only the selected sentence without any additional explanation or phrases:\\n\"\n",
    "    prompt += \"\\n\".join([f\"{i+1}. {trans}\" for i, trans in enumerate(transcriptions)])\n",
    "\n",
    "    try:\n",
    "        response = gpt_request(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a text refinement assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        ).choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error with prompt: {prompt}\\nError: {e}\")\n",
    "        response = transcriptions[0]  # Default to the first option if there's an error\n",
    "    \n",
    "    # Ensure the response does not include numbering or additional phrases\n",
    "    response = response.replace(\"The most comprehensive and coherent sentence is:\", \"\").strip()\n",
    "    \n",
    "    # Remove any backslashes\n",
    "    response = response.replace(\"\\\\\", \"\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "def gpt_ensemble_transcription(sentence, min_length=5):\n",
    "    filtered_transcriptions = get_filtered_transcriptions(sentence, min_length)\n",
    "    refined_transcription = refine_transcription(filtered_transcriptions)\n",
    "    return refined_transcription\n",
    "\n",
    "def process_session(data, session_name):\n",
    "    # Filter data for the specific session\n",
    "    session_data = [item for item in data if item['id'].startswith(session_name)]\n",
    "    \n",
    "    # Process each script within the session\n",
    "    processed_data = []\n",
    "    for i, item in enumerate(session_data):\n",
    "        try:\n",
    "            item['ensemble'] = gpt_ensemble_transcription(item)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing record {i+1} in session {session_name}: {e}\")\n",
    "            item['ensemble'] = None  # Indicate failure to process this item\n",
    "        processed_data.append(item)\n",
    "        \n",
    "        if (i + 1) % 10 == 0 or i == len(session_data) - 1:\n",
    "            print(f\"Processed {i + 1}/{len(session_data)} records in session {session_name}\")\n",
    "    \n",
    "    # Save the updated JSON file for the session\n",
    "    output_file = f'F:\\\\SLT_2024\\\\baseline\\\\{session_name}_processed.json'\n",
    "    with open(output_file, 'w') as file:\n",
    "        json.dump(processed_data, file, indent=4)\n",
    "\n",
    "def apply_ensemble_method_for_session(file_path, session_name):\n",
    "    # Load the JSON file\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Process the specified session\n",
    "    process_session(data, session_name)\n",
    "\n",
    "# Apply the ensemble method to a specific session\n",
    "file_path = 'F:\\\\SLT_2024\\\\baseline\\\\emotion_eval.json'\n",
    "session_name = 'Ses05E'  # Change this to process a different session\n",
    "apply_ensemble_method_for_session(file_path, session_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Combine all processed sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Define the order of the sessions\n",
    "session_order = [\n",
    "    \"Ses01Z\", \"Ses01E\", \"Ses02Z\", \"Ses02E\", \n",
    "    \"Ses03Z\", \"Ses03E\", \"Ses04Z\", \"Ses04E\", \n",
    "    \"Ses05Z\", \"Ses05E\"\n",
    "]\n",
    "\n",
    "# Define the directory where the processed JSON files are stored\n",
    "input_dir = \"F:\\\\SLT_2024\\\\baseline\\\\\"\n",
    "\n",
    "# Initialize a list to hold all the combined data\n",
    "combined_data = []\n",
    "\n",
    "# Loop through each session in the specified order\n",
    "for session_name in session_order:\n",
    "    # Define the file path for the current session\n",
    "    file_path = os.path.join(input_dir, f\"{session_name}_processed.json\")\n",
    "    \n",
    "    # Load the JSON data for the current session\n",
    "    with open(file_path, 'r') as file:\n",
    "        session_data = json.load(file)\n",
    "    \n",
    "    # Add the session data to the combined data list\n",
    "    combined_data.extend(session_data)\n",
    "\n",
    "# Define the output file path for the combined JSON file\n",
    "output_file_path = os.path.join(input_dir, \"F:\\\\SLT_2024\\\\baseline\\\\CombinedSessions\\\\combined_sessions.json\")\n",
    "\n",
    "# Save the combined data to the output JSON file\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    json.dump(combined_data, output_file, indent=4)\n",
    "\n",
    "print(f\"Combined JSON file created at: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/1034 records in session Ses05\n",
      "Processed 20/1034 records in session Ses05\n",
      "Processed 30/1034 records in session Ses05\n",
      "Processed 40/1034 records in session Ses05\n",
      "Processed 50/1034 records in session Ses05\n",
      "Processed 60/1034 records in session Ses05\n",
      "Processed 70/1034 records in session Ses05\n",
      "Processed 80/1034 records in session Ses05\n",
      "Processed 90/1034 records in session Ses05\n",
      "Processed 100/1034 records in session Ses05\n",
      "Processed 110/1034 records in session Ses05\n",
      "Processed 120/1034 records in session Ses05\n",
      "Processed 130/1034 records in session Ses05\n",
      "Processed 140/1034 records in session Ses05\n",
      "Processed 150/1034 records in session Ses05\n",
      "Processed 160/1034 records in session Ses05\n",
      "Processed 170/1034 records in session Ses05\n",
      "Processed 180/1034 records in session Ses05\n",
      "Processed 190/1034 records in session Ses05\n",
      "Processed 200/1034 records in session Ses05\n",
      "Processed 210/1034 records in session Ses05\n",
      "Processed 220/1034 records in session Ses05\n",
      "Processed 230/1034 records in session Ses05\n",
      "Processed 240/1034 records in session Ses05\n",
      "Processed 250/1034 records in session Ses05\n",
      "Processed 260/1034 records in session Ses05\n",
      "Processed 270/1034 records in session Ses05\n",
      "Processed 280/1034 records in session Ses05\n",
      "Processed 290/1034 records in session Ses05\n",
      "Processed 300/1034 records in session Ses05\n",
      "Processed 310/1034 records in session Ses05\n",
      "Processed 320/1034 records in session Ses05\n",
      "Processed 330/1034 records in session Ses05\n",
      "Processed 340/1034 records in session Ses05\n",
      "Processed 350/1034 records in session Ses05\n",
      "Processed 360/1034 records in session Ses05\n",
      "Processed 370/1034 records in session Ses05\n",
      "Processed 380/1034 records in session Ses05\n",
      "Processed 390/1034 records in session Ses05\n",
      "Processed 400/1034 records in session Ses05\n",
      "Processed 410/1034 records in session Ses05\n",
      "Processed 420/1034 records in session Ses05\n",
      "Processed 430/1034 records in session Ses05\n",
      "Processed 440/1034 records in session Ses05\n",
      "Processed 450/1034 records in session Ses05\n",
      "Processed 460/1034 records in session Ses05\n",
      "Processed 470/1034 records in session Ses05\n",
      "Processed 480/1034 records in session Ses05\n",
      "Processed 490/1034 records in session Ses05\n",
      "Processed 500/1034 records in session Ses05\n",
      "Processed 510/1034 records in session Ses05\n",
      "Processed 520/1034 records in session Ses05\n",
      "Processed 530/1034 records in session Ses05\n",
      "Processed 540/1034 records in session Ses05\n",
      "Processed 550/1034 records in session Ses05\n",
      "Processed 560/1034 records in session Ses05\n",
      "Processed 570/1034 records in session Ses05\n",
      "Processed 580/1034 records in session Ses05\n",
      "Processed 590/1034 records in session Ses05\n",
      "Processed 600/1034 records in session Ses05\n",
      "Processed 610/1034 records in session Ses05\n",
      "Processed 620/1034 records in session Ses05\n",
      "Processed 630/1034 records in session Ses05\n",
      "Processed 640/1034 records in session Ses05\n",
      "Processed 650/1034 records in session Ses05\n",
      "Processed 660/1034 records in session Ses05\n",
      "Processed 670/1034 records in session Ses05\n",
      "Processed 680/1034 records in session Ses05\n",
      "Processed 690/1034 records in session Ses05\n",
      "Processed 700/1034 records in session Ses05\n",
      "Processed 710/1034 records in session Ses05\n",
      "Processed 720/1034 records in session Ses05\n",
      "Processed 730/1034 records in session Ses05\n",
      "Processed 740/1034 records in session Ses05\n",
      "Processed 750/1034 records in session Ses05\n",
      "Processed 760/1034 records in session Ses05\n",
      "Processed 770/1034 records in session Ses05\n",
      "Processed 780/1034 records in session Ses05\n",
      "Processed 790/1034 records in session Ses05\n",
      "Processed 800/1034 records in session Ses05\n",
      "Processed 810/1034 records in session Ses05\n",
      "Processed 820/1034 records in session Ses05\n",
      "Processed 830/1034 records in session Ses05\n",
      "Processed 840/1034 records in session Ses05\n",
      "Processed 850/1034 records in session Ses05\n",
      "Processed 860/1034 records in session Ses05\n",
      "Processed 870/1034 records in session Ses05\n",
      "Processed 880/1034 records in session Ses05\n",
      "Processed 890/1034 records in session Ses05\n",
      "Processed 900/1034 records in session Ses05\n",
      "Processed 910/1034 records in session Ses05\n",
      "Processed 920/1034 records in session Ses05\n",
      "Processed 930/1034 records in session Ses05\n",
      "Processed 940/1034 records in session Ses05\n",
      "Processed 950/1034 records in session Ses05\n",
      "Processed 960/1034 records in session Ses05\n",
      "Processed 970/1034 records in session Ses05\n",
      "Processed 980/1034 records in session Ses05\n",
      "Processed 990/1034 records in session Ses05\n",
      "Processed 1000/1034 records in session Ses05\n",
      "Processed 1010/1034 records in session Ses05\n",
      "Processed 1020/1034 records in session Ses05\n",
      "Processed 1030/1034 records in session Ses05\n",
      "Processed 1034/1034 records in session Ses05\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "import time\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-'\n",
    "\n",
    "client = OpenAI()\n",
    "log = []\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(10))\n",
    "def gpt_request(**kwargs):\n",
    "    return client.chat.completions.create(**kwargs)\n",
    "\n",
    "def get_filtered_transcriptions(sentence, min_length=5):\n",
    "    transcriptions = [\n",
    "        sentence['hubertlarge'],\n",
    "        sentence['w2v2100'],\n",
    "        sentence['w2v2960'],\n",
    "        sentence['w2v2960large'],\n",
    "        sentence['w2v2960largeself'],\n",
    "        sentence['wavlmplus'],\n",
    "        sentence['whisperbase'],\n",
    "        sentence['whisperlarge'],\n",
    "        sentence['whispermedium'],\n",
    "        sentence['whispersmall'],\n",
    "        sentence['whispertiny']\n",
    "    ]\n",
    "    \n",
    "    # Filter out very short responses\n",
    "    filtered_transcriptions = [trans for trans in transcriptions if len(trans) > min_length]\n",
    "    \n",
    "    # If all transcriptions are short, return all available transcriptions\n",
    "    if not filtered_transcriptions:\n",
    "        return transcriptions\n",
    "    \n",
    "    return filtered_transcriptions\n",
    "\n",
    "def refine_transcription(transcriptions):\n",
    "    prompt = \"Choose the most comprehensive and coherent sentence from the following options. If impossible to decide, choose the longest option available. Output only the selected sentence without any additional explanation or phrases:\\n\"\n",
    "    prompt += \"\\n\".join([f\"{i+1}. {trans}\" for i, trans in enumerate(transcriptions)])\n",
    "\n",
    "    try:\n",
    "        response = gpt_request(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a text refinement assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        ).choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error with prompt: {prompt}\\nError: {e}\")\n",
    "        response = transcriptions[0]  # Default to the first option if there's an error\n",
    "    \n",
    "    # Ensure the response does not include numbering or additional phrases\n",
    "    response = response.replace(\"The most comprehensive and coherent sentence is:\", \"\").strip()\n",
    "    \n",
    "    # Remove any backslashes\n",
    "    response = response.replace(\"\\\\\", \"\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "def gpt_ensemble_transcription(sentence, min_length=5):\n",
    "    filtered_transcriptions = get_filtered_transcriptions(sentence, min_length)\n",
    "    refined_transcription = refine_transcription(filtered_transcriptions)\n",
    "    return refined_transcription\n",
    "\n",
    "def process_session(data, session_name):\n",
    "    # Filter data for the specified session\n",
    "    session_data = [item for item in data if item['id'].startswith(session_name)]\n",
    "    \n",
    "    # Process each script within the session\n",
    "    processed_data = []\n",
    "    for i, item in enumerate(session_data):\n",
    "        try:\n",
    "            item['ensemble'] = gpt_ensemble_transcription(item)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing record {i+1} in session {session_name}: {e}\")\n",
    "            item['ensemble'] = None  # Indicate failure to process this item\n",
    "        processed_data.append(item)\n",
    "        \n",
    "        if (i + 1) % 10 == 0 or i == len(session_data) - 1:\n",
    "            print(f\"Processed {i + 1}/{len(session_data)} records in session {session_name}\")\n",
    "    \n",
    "    # Save the updated JSON file for the session\n",
    "    output_file = f'F:\\\\SLT_2024\\\\train_ensemble\\\\{session_name}_processed.json'\n",
    "    with open(output_file, 'w') as file:\n",
    "        json.dump(processed_data, file, indent=4)\n",
    "\n",
    "def apply_ensemble_method_for_session(file_path, session_name):\n",
    "    # Load the JSON file\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    # Process the specified session\n",
    "    process_session(data, session_name)\n",
    "\n",
    "# Apply the ensemble method to a specific session\n",
    "file_path = 'F:\\\\SLT_2024\\\\train_ensemble\\\\iemocap_script_allemo.json'\n",
    "session_name = 'Ses05'  # Change this to process a different session\n",
    "apply_ensemble_method_for_session(file_path, session_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Combine all processed sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Define the order of the sessions\n",
    "session_order = [\n",
    "    \"Ses01\", \"Ses02\", \"Ses03\", \"Ses04\", \"Ses05\"\n",
    "]\n",
    "\n",
    "# Define the directory where the processed JSON files are stored\n",
    "input_dir = \"F:\\\\SLT_2024\\\\train_ensemble\\\\\"\n",
    "\n",
    "# Initialize a list to hold all the combined data\n",
    "combined_data = []\n",
    "\n",
    "\n",
    "for session_name in session_order:\n",
    "    \n",
    "    file_path = os.path.join(input_dir, f\"{session_name}_processed.json\")\n",
    "    \n",
    "    # Load the JSON data for the current session\n",
    "    with open(file_path, 'r') as file:\n",
    "        session_data = json.load(file)\n",
    "    \n",
    "    # Add the session data to the combined data list\n",
    "    combined_data.extend(session_data)\n",
    "\n",
    "# Define the output file path for the combined JSON file\n",
    "output_file_path = os.path.join(input_dir, \"F:\\\\SLT_2024\\\\train_ensemble\\\\combined\\\\combined.json\")\n",
    "\n",
    "\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    json.dump(combined_data, output_file, indent=4)\n",
    "\n",
    "print(f\"Combined JSON file created at: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
